---
title: "shapley"
author: "Vignette Author"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
vignette: |
  %\VignetteIndexEntry{shapley} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE,results='hide'}
library(ggplot2)
#library(shapleyr)
#load the packages that we need
devtools::load_all()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(collapse = T, comment = "#>")
```

## R Shapley
The Shapley value is a method that gives a solution to the following problem: A coalition of players play a game, for which they get a payout, but it is not clear how to distribute the payout fairly among the players. The Shapley value solves this problem by trying out different coalitions of players to decide how much each player changes the amount of the payout. 
What does this have to do with machine learning? In machine learning, the features (=players) work together to get the payout (=predicted value). The Shapley value tells us, how much each feature contributed to the prediction.
Properties:
  * Pareto-efficiency.
  * Symmetry.
  * Additivity.
  * Zero player.

# Shapley function
row.nr = 3 is just an example. You can also choose a range like row.nr = 1:5

###example1: 
### regression task
```{r}
#   task = bh.task,      predict.methods = regr.lm
s1 <-shapley(row.nr = 3, task = bh.task, model = train(makeLearner("regr.lm"), bh.task))
# The Shapley function gives you as output many informations like task type, feature names, predict type, prediction response, mean of data and the shapley values for every feature. You get the information by using the get functions, which are described below. Here you can see the whole output:
s1

# Here we use getShapleyValues(s1) to show only the shapley values. In the following examples we show only the shapley values. 
knitr::kable(getShapleyValues(s1))
```

###example2:
### classification task
```{r}
#  task = iris.task,      predict.methods = classif.lda
s2 <-shapley(row.nr = 3, task = iris.task, model = train(makeLearner("classif.lda"), iris.task))
# shapley Value
knitr::kable(getShapleyValues(s2))
```

###example3:
### multilabel task
```{r}
#  task = yeast.task,      predict.methods = multilabel.rFerns
s3 <-shapley(row.nr = 3, task = yeast.task, model = train(makeLearner("multilabel.rFerns"), yeast.task))
# shapley Value
knitr::kable(getShapleyValues(s3))
```

###example4:
### cluster task
```{r}
#  task = mtcars.task,      predict.methods = cluster.kmeans
s4 <-shapley(row.nr = 5, task = mtcars.task, model = train(makeLearner("cluster.kmeans"), mtcars.task))
# shapley Value
knitr::kable(getShapleyValues(s4))
```

###example5:
### calculate shapley value without sampling (not for mlr or tasks with a lot features)
```{r}
### Unsampled version created for calculating the exact shapley value.
### A lot to calculate, because every permutation needs to be calculated
s5 <-shapley.unsampled()

```

An example how to calculate the Shapley value of feature b:

Permutationen        Beitrag vor b     vor und mit b    Marginaler Beitrag
-------------       --------------     --------------   -------------------
        a,b,c       v({a}) = 12         v({a,b})=24              12 = 24-12
        a,c,b       v({a,c}) = 27       v({a,b,c}) = 36           9 = 36-27
        b,a,c       v({}) = 0           v({b}) = 6                6
        b,c,a       v({}) = 0           v({b}) = 6                6
        c,a,b       v({a,c}) = 27       v({a,b,c}) = 36           9
        c,b,a       v({c}) = 9          v({b,c}) = 15             6
-------------       --------------     --------------  --------------------
Result: Sh_b({a,b,c}, v) = 8
Same for Sh_a = 17, Sh_c = 11



# Including Plots
###example1:
### show the influence of one single value
```{r}
s1 <-shapley(row.nr = 3, task = bh.task, model = train(makeLearner("regr.lm"), bh.task))
plot.shapley.singleValue(s1)
```
###example3:
### show the multifeatures influence
```{r}
shap.values = shapley(1:3, task = bh.task, model = train(makeLearner("regr.lm"), bh.task))
plot.shapley.multipleFeatures(shap.values)
#plot.shapley.multipleFeatures(1:2,features = c("crim","rad","tax","nox"))
```
```{r}

```

```{r, echo = FALSE}
#knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

# Convergence
Tests that the shapley algorithm converges
parameters are : 
    row.nr Index for the observation of interest.
    convergence.iterations Amount of iterations of the shapley function.
    iterations Amount of the iterations within the shapley
    function.
    return.value You can choose between plotting results or getting a data frame ("plot", "values")
return shapley value as a data.frame with col.names and their corresponding
effects.
```{r, echo = TRUE}
#compares the result of the shapley function with the prediction (from row number)
#shows how many times a value from the shapley function was chosen.
test.convergence(row.nr = 2, convergence.iterations = 20, iterations = 20, task = mtcars.task,
                            model = train(makeLearner("cluster.kmeans"), mtcars.task),
                            return.value = "values")
#The class, which is shown like this <<classname>>, is the class which was predicted by the model (also for clustering). Below the classname is shown how many times this class was chosen by the shapley function, because it has the biggest shapley value.

```

```{r}

#plot the convergence
#test.convergence(return.value = "plot", ...)
#get a data frame
#test.convergence(return.value = "values")
#change amount of iterations in the shapley function
#test.convergence(iterations = ...)
#change amount of calls of shapey function
#test.convergence(convergence.iterations = ...)
#choose observation as reference, for example an observation that you know is normal
#test.convergence(row.nr = ...)

```
 


